## 关于深度学习的其他问题

### dropout
* Dropout什么原理
dropout实现一种继承的学习思想，在每次训练的时候以一定概率p 丢失一个节点，每次丢失的节点都不同，使得每次训练的模型都是独一无二的，最终集成于一个模型中。
在集成时候是按照测试时的权重乘以概率p。

工作原理：以一个概率为p的伯努利分布随机的生成与节点数相同的0，1值，将这些值与输入相乘后部分节点被屏蔽，此时再用这些节点值做后续的计算。
![](https://img-blog.csdnimg.cn/20200201094927792.png)

* dropout的底层原理(同上)
Dropout层中为什么dropout后还需要进行rescale
既然 rescale 是为了“保持期望不变”，那么为什么要“保持期望不变”呢？
因为在测试数据中不会进行dropout。dropout是带有随机性的，如果 测试集 也做的话，网络的输出就不稳定。
在 infer 不做 dropout 的前提下，保证训练和预测过程的分布一致，而对 测试集 进行 rescale。

dropout 有两种实现方式，Vanilla Dropout 和 Inverted Dropout。
![](other_files/1.jpg)

在Vanilla版本中，在 forward 时直接对输入进行遮罩即X = [x0,0,x2,x3],在反传的时候D = [d0,0,d2,d3]。在测试数据集上进行1-p的缩小 即I=0.8X。为了
保持训练集和测试集的期望一致。
**Vanilla中存在的问题 预测过程需要跟着 dropout 策略做调整，哪些层取消了、加重了或者减轻了，都需要改。**

在Inverted中 在forward和backward 时候放大了期望，即先遮罩再进行 1/(1-p) 的放大   X=1.25* [x0,0,x2,x3] backward过程也是一样D =1.25* [d0,0,d2,d3]
在训练数据集上不做任何处理。

* 参数初始化的作用是啥

* 参数初始化不能全为0？
因为所有参数都是0的话，那么所有神经元的输出都是一样的了，那么在back propagation的时候同一层的所有神经元的gradient都相同，参数更新也就相同，那就无法学习
到模型参数。

* 说一说偏差和方差吧，
* 过拟合的解决办法

* 数据不平衡怎么办，一般什么场合会出现数据不平衡
**数据不平衡怎么办**
1、从数据的角度，我们可以对于少量的样本，尽可能的扩大其数据集，或者增加他们特有的特征来丰富数据的多样性。
2、采样。
	欠采样(下采样),通过对丰富样本的进行较少采样，即在丰富类别样本中进行随机采样  与稀有样本相等数量的样本。
	过采样(上采样)SMOTE采样方法:从少数类创建新的合成点，以增加其基数。即通过增加稀有样本的数量来平衡数据集。**通过使用重复、自举或合成少数类过采样等方法（SMOTE）来生成新的稀有样品。**
3、加权
	这个方式主要是将大类样本进行分割，分割成小的不相交子集。分割多小呢，按照不同类别所占的比例，也就及两个小类样本数量差不多。
	这样将子集和小类样本进行结合，训练子分类器，最后按照多个字分类器进行组合。

4、算法方面
	集成方法：平衡随机森林的方法，该方法对正类和反类分别进行重采样，重采样多次后采用多数投票的方法进行集成学习。
	代价敏感：通过对错分稀有类的样本付出更大的代价，通过最小化损失来控制不平衡分类的问题。通过损失函数，使得模型对稀有类更有区分度。
5、一分类问题
	对于数据不平衡问题，我们让模型只进行二分类，即检测出一类，剩下的归为另一类，这类方法的重点不在于捕捉类间的差别，而是为其中一类进行建模。
	这个方法不适用于一个大类  其他的都是稀有类。
总的来说长做的方式就是进行采样，数据的合成，加权等方法。
**在正负样本都非常之少的情况下，应该采用数据合成的方式**
**在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法（癌症检测）；**
**在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。**
	
* 除了梯度下降，还有啥优化方法？为啥不用牛顿法呢？
* 常见的采样方法？
* 过拟合的解决[https://blog.csdn.net/u014465639/article/details/71265492](https://blog.csdn.net/u014465639/article/details/71265492)
early stopping、数据集扩增（Data augmentation）、正则化（Regularization）、Dropout等。
1、early stopping：Early stopping方法的具体做法是，在每一个Epoch结束时，计算validation data的accuracy，当accuracy不再提高时，就停止训练。
	但是具体多少次epoch没提高就算正确率不提高了，这个可以人为规定。(当连续10次Epoch（或者更多次）没达到最佳accuracy时，则可以认为accuracy不再提高了。此时便可以停止迭代了)

2、数据集扩增
3、正则化：在目标函数上加上l1或l2正则，
4、dropout：通过伯努利分布随机使模型中的一些节点不进行参数的更新，也就是说减少了学习，使得模型的泛化能力更强，也就是降低多
	原始数据学的太仔细，学到太多的噪音  而导致过拟合。

* Normalization  [https://zhuanlan.zhihu.com/p/43200897](https://zhuanlan.zhihu.com/p/43200897)