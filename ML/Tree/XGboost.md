## 决策树之XGboost

### 从GBDT到XGBoost
1、算法本身的优化
	1）学习器的选择，对比GBDT只支持决策树来说，还可以支持很多其他弱学习器。
	2）在损失函数上，除了本身的损失，还加上了正则化部分。
	3）优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。
2、算法运行效率的优化
	对于每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，
	方便前面说的并行选择。
3、算法健壮性的优化
	1）对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。
	2）加入L1和L2正则化，防止过拟合

### XGBoost损失函数
对于GBDT的求解过程，需要解决当前决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解$c_tj$。
GBDT采样的方法是分两步走，先求出最优的所有J个叶子节点区域，再求出每个叶子节点区域的最优解。

XGBoost的损失函数的形式。
![](XGboost_files/1.jpg)
这里的J是叶子节点的个数，而wtj是第j个叶子节点的最优值。这里的wtj和我们GBDT里使用的ctj是一个意思，
只是XGBoost的论文里用的是w表示叶子区域的值。

1. XGboost 的节点分裂时候，依靠什么？数学形式？XGboost 比GBDT好在哪？
2. xgb和lgb的区别
3. xgboost和lightgbm的区别
4. 对xgboost的了解
5. xgboost的二阶泰勒展开为啥那么效果更好，
6. 高维稀疏的特征为啥不适合用xgboost