## 决策树之XGboost

### 从GBDT到XGBoost
1、算法本身的优化
	1）学习器的选择，对比GBDT只支持决策树来说，还可以支持很多其他弱学习器。
	2）在损失函数上，除了本身的损失，还加上了正则化部分。
	3）优化方式上，GBDT的损失函数只对误差部分做负梯度（一阶泰勒）展开，而XGBoost损失函数对误差部分做二阶泰勒展开，更加准确。
2、算法运行效率的优化
	对于每个弱学习器，比如决策树建立的过程做并行选择，找到合适的子树分裂特征和特征值。在并行选择之前，先对所有的特征的值进行排序分组，
	方便前面说的并行选择。
3、算法健壮性的优化
	1）对于缺失值的特征，通过枚举所有缺失值在当前节点是进入左子树还是右子树来决定缺失值的处理方式。
	2）加入L1和L2正则化，防止过拟合

### XGBoost损失函数
对于GBDT的求解过程，需要解决当前决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解$c_tj$。
GBDT采样的方法是分两步走，先求出最优的所有J个叶子节点区域，再求出每个叶子节点区域的最优解。

XGBoost的损失函数的形式。
![](XGboost_files/1.jpg)
这里的J是叶子节点的个数，而wtj是第j个叶子节点的最优值。这里的wtj和我们GBDT里使用的ctj是一个意思，
只是XGBoost的论文里用的是w表示叶子区域的值。
![](XGboost_files/2.jpg)
最终我们要极小化上面这个损失函数，得到第t个决策树最优的所有J个叶子节点区域和每个叶子节点区域的最优解wtj。

不同：
XGBoost没有和GBDT一样去拟合泰勒展开式的一阶导数，而是期望直接基于损失函数的二阶泰勒展开式来求解。

因此通过二阶展开式的损失函数的最终表示：
![](XGboost_files/3.jpg)
可以参考[刘建平](https://www.cnblogs.com/pinard/p/10979808.html)博客的推导过程

### XGBoost损失函数的优化求解
对于一次性求出损失函数的最优解，可以分为2个子问题
1) 如果我们已经求出了第t个决策树的J个最优的叶子节点区域，如何求出每个叶子节点区域的最优解wtj？
我们直接基于损失函数对wtj求导并令导数为0即可。这样我们得到叶子节点区域的最优解wtj表达式为：
![](XGboost_files/5.jpg)
![](XGboost_files/4.jpg)
其实这个过程已经在GBDT中已经提出过了，可以参考刘建平老师的博客

2) 对当前决策树做子树分裂决策时，应该如何选择哪个特征和特征值进行分裂，使最终我们的损失函数Lt最小？
在GBDT里面，我们是直接拟合的CART回归树，所以树节点分裂使用的是均方误差。XGBoost这里不使用均方误差，
而是使用贪心法，即每次分裂都期望最小化我们的损失函数的误差。
![](XGboost_files/6.jpg)
就是将
![](XGboost_files/4.jpg)
带入到损失函数即可，
![](XGboost_files/8.jpg)

什么意思呢，就是在分裂子树的过程中，通过不断计算不同的分裂节点，可以得到GL和HL的值，这样最大化上述式子的结果，则可以确定分裂子树的点。
这不同于GBDT中通过均方差来寻找分裂点。

### 主要流程
![](XGboost_files/7.jpg)

### XGBoost算法运行效率的优化
**运行效率的优化**
1、Boosting算法的弱学习器是没法并行迭代的，但是单个弱学习器里面最耗时的是决策树的分裂过程，XGBoost针对这个分裂做了比较大的并行优化。
	对于不同的特征的特征划分点，XGBoost分别在不同的线程中并行选择分裂的最大增益。
	
2、对训练的每个特征排序并且以块的的结构存储在内存中，方便后面迭代重复使用，减少计算量。
	首先默认所有的样本都在右子树，然后从小到大迭代，依次放入左子树，并寻找最优的分裂点。这样做可以减少很多不必要的比较。

**健壮性的优化**
1、加入正则化的方式，提高其泛化能力

2、**XGBoost没有假设缺失值一定进入左子树还是右子树，则是尝试通过枚举所有缺失值在当前节点是进入左子树，还是进入右子树更优来决定一个处理缺失值默认的方向，这样处理起来更加的灵活和合理。**


1. XGboost 的节点分裂时候，依靠什么？数学形式？XGboost 比GBDT好在哪？
	通过贪心的思想，使得每次分裂子树的时候期望值最小化损失函数的误差，
	
2. xgb和lgb的区别
3. xgboost和lightgbm的区别
	还没看过lightgbm
	
4. 对xgboost的了解
	
5. xgboost的二阶泰勒展开为啥那么效果更好，
	1、因为对于MSE来说其二阶展开是一种很好的形式(一阶+二阶)，但是对于其他损失函数(logloss)来说就没这么好了，因此为了统一都采用
	泰勒公式的二阶展开式。简而言之：就是为了统一损失函数求导形式以满足自定义的损失函数。
	
	2、二阶泰勒展开本身对于梯度的收敛更快更准确。因为从二阶导本身的性质来说，一阶导数简单的认为是梯度方向，二阶导则是指引梯度方向如何变化
	
6. 高维稀疏的特征为啥不适合用xgboost
	因为树模型在训练过程中是一个贪心思想选择特征的过程，需要寻找一个分裂后信息增益最大的特征进行分类。如果使用高维稀疏特征，会使得子树的
	数量增多，计算量增大，导致模型的训练过程费时。
	
	除此之外由于高维稀疏特征对于树模型来说更容易发生过拟合的情况，因为对于树模型来说，我们的
	正则项通常是树的叶子和树的深度，但是对于高维稀疏特征，也只需要一个节点就可以分裂，惩罚不够，而对于线性模型，正则项通常是对权重W进行惩罚，
	因此对于每个维度可以进行控制，这样可以控制权重的大小。