## 关于决策树的其他类问题

1. 如何用回归树做分类
2. 决策树是怎么样进行划分的，决策树的损失函数
3. 你觉得树模型和神经网络的数据预处理有什么不同吗？
4. 决策树是怎么样进行划分的，决策树的损失函数
5. 树模型的推导

6. Bagging和Boosting
	Bagging是Bootstrap aggregating的缩写。中文意思是自助聚合。而Bootstrap本身是一种有放回的抽样方法（可能抽到重复的样本）。
	1、从原始样本集中随机采样。每轮从原始样本集中有放回的选取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。
		共进行k轮抽取，得到k个训练集。（bootstrap的过程，由于是有放回抽样，所以k个训练集之间相互独立）
	2、每次使用一份训练集训练一个模型，k 个训练集共得到 k 个基模型。（注：这里并没有具体的分类算法或回归方法，
		我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
	3、利用这k个基模型对测试集进行预测，将k个预测结果进行聚合。(aggregating的过程)
		1、分类问题：将上步得到的k个模型采用投票的方式得到分类结果
		2、回归问题：计算上述模型的均值作为最后的结果。（所有模型的重要性相同）
	Bagging算法特点：
	1、 可并行的集成方法。每个基模型可以分别、独立、互不影响地生成。
	2、 主要降低 Variance，对 Bias 无明显作用。因此，适用于 High Variance & Low Bias 的模型。
		偏差角度：Bagging后的偏差与单个模型相近。
			由于$Bias = \frac{1}{k} \sum_{i=1}^k bias_i$ ( 共k个基模型，$bias_i$ 为第 i 个基模型的偏差)，所以 Bagging后的偏差与单个模型相近。
		方差角度：
			整体的方差是 $Var = \frac{\sigmoid ^2}{n}$.因此Bagging可以减少方差。
			
	Boosting的核心思想是将多个弱分类器组装成一个强分类器 。
	
	关于Boosting的两个核心问题：
		1、在每一轮如何改变训练数据的权值或概率分布？
			通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。
		2、通过什么方式来组合弱分类器？
			通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，
			同时减小错误率较大的分类器的权值。提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。
	
	Boosting算法特点：
	
	1、序贯式集成方法（Sequential Ensemble）。每轮迭代生成的基模型，主要提升前一代基模型表现不好的地方。
	2、不断迭代弱分类器，从而降低 Bias。因此，适用于 Low Variance & High Bias 的模型。
	
	基于Boosting算法典型代表：如AdaBoost、GBDT、XGBoost等。

7. Bagging，Boosting二者之间的区别
	1）样本选择：
	Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。
	Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

	2）样例权重：
	Bagging：使用均匀取样，每个样例的权重相等
	Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

	3）预测函数：
	Bagging：所有预测函数的权重相等。
	Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

	4）并行计算：
	Bagging：各个预测函数可以并行生成
	Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

		